{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Load dữ liệu giá nhà"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name):\n",
    "    dat = pd.read_csv(file_name)\n",
    "    N = dat.shape[0]\n",
    "\n",
    "    yrs_old = np.zeros(N)\n",
    "    yrs_reno = np.zeros(N)\n",
    "    for i in range(N):\n",
    "        year_sell = int(dat['date'][i][-4:])\n",
    "        yrs_old[i] = year_sell - dat['yr_built'][i]\n",
    "        if dat['yr_renovated'][i] == 0:\n",
    "            yrs_reno[i] = 0\n",
    "        else:\n",
    "            yrs_reno[i] = year_sell - dat['yr_renovated'][i]\n",
    "\n",
    "    y = np.array(dat['price']).reshape((N, 1)) * 10e-6\n",
    "\n",
    "    yrs_old = yrs_old.reshape((N, 1))\n",
    "    yrs_reno = yrs_reno.reshape((N, 1))\n",
    "    X = dat.to_numpy(dat.drop(columns=['id', 'date', 'price', 'yr_built', \\\n",
    "                                       'yr_renovated'], inplace=True)) # 'zipcode', 'lat', 'long\n",
    "    X = np.concatenate((X, yrs_old, yrs_reno), axis=1)\n",
    "    X = (X - np.min(X, axis=0)) / (np.max(X, axis=0) - np.min(X, axis=0))\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X, y = load_data('kc_house_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17277, 18)\n",
      "(17277, 1)\n",
      "(4320, 18)\n",
      "(4320, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Các hàm và thư viện cần thiết"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_cost_curve(model):\n",
    "    cost_t_values = model.cost_t_values\n",
    "    cost_v_values = model.cost_v_values\n",
    "    plt.plot(cost_t_values)\n",
    "    plt.plot(cost_v_values)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conclusion(model, score=None):\n",
    "    if score is None: score = model.score\n",
    "    print(\"===================\")\n",
    "    print(\"R^2 score on the training set: \", score(X_train, y_train))\n",
    "    print(\"R^2 score on the testing set: \", score(X_test, y_test))\n",
    "    print(\"Time for fitting: \", model.fitting_time)\n",
    "    if model.benchmark_enabled:\n",
    "        print(\"Cost on the training set: \", model.cost_t_values[-1])\n",
    "        print(\"Cost on the testing set: \", model.cost_v_values[-1])\n",
    "        draw_cost_curve(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Sử dụng phương pháp tối ưu & mô hình Hồi quy tuyến tính từ thư viện Scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression as SKLinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "19d364d29be43bc856b31f0ddec1d5a50a88e2c5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression(n_jobs=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression(n_jobs=2)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression(n_jobs=2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SKLinearRegression(n_jobs=2)\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE :  4.020510526092811\n",
      "Score :  0.6912436353894154\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"MSE : \",mean_squared_error(y_pred , y_test))\n",
    "print(\"Score : \",r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Tự xây dựng các thuật toán tối ưu Gradient Descent, Stochastic Gradient Descent và Mini Batch Gradient Descent cho mô hình Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    def __init__(self, optimizer, step_size, steps=100, benchmark_enabled=True, \n",
    "                 alpha=0.5, beta=0.5, backtracking_enabled=False, tol=10e-4, early_stopping_enabled=False):\n",
    "        self.optimizer = optimizer\n",
    "        self.step_size = step_size\n",
    "        self.steps = steps\n",
    "        self.params = None\n",
    "        self.cost_t_values = list()\n",
    "        self.cost_v_values = list()\n",
    "        self.fitting_time = None\n",
    "        self.benchmark_enabled = benchmark_enabled\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.backtracking_enabled = backtracking_enabled\n",
    "        self.early_stopping_enabled = early_stopping_enabled\n",
    "        self.tol = tol\n",
    "        \n",
    "    def fit(self, X_train, y_train, X_test, y_test):\n",
    "        start_time = time.time()\n",
    "        X_train = np.concatenate((X_train, np.ones((X_train.shape[0], 1))), axis=1)\n",
    "        X_test = np.concatenate((X_test, np.ones((X_test.shape[0], 1))), axis=1)\n",
    "        self.params = self.optimizer(X_train, y_train, X_test, y_test, self)\n",
    "        self.fitting_time = time.time() - start_time\n",
    "        \n",
    "    def predict(self, X):\n",
    "        X = np.concatenate((X, np.ones((X.shape[0], 1))), axis=1)\n",
    "        y_pred = np.dot(X, self.params)\n",
    "        return y_pred\n",
    "    \n",
    "    @staticmethod\n",
    "    def gradient(X, y_pred, y):\n",
    "        N = y.shape[0]\n",
    "        return 2 / N * np.dot(X.transpose(),(y_pred - y))\n",
    "    \n",
    "    @staticmethod\n",
    "    def hessian(X):\n",
    "        N = X.shape[0]\n",
    "        return np.squeeze(2 / N * np.dot(X.transpose(), X))[()]\n",
    "    \n",
    "    @staticmethod\n",
    "    def cost(y, y_pred):\n",
    "        N = y.shape[0]\n",
    "        residual = y - y_pred\n",
    "        return 1/N * np.squeeze(np.dot(residual.transpose(), residual))[()]\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        residual = y - y_pred\n",
    "        rss = np.squeeze(np.dot(residual.transpose(), residual))[()]\n",
    "        y_mean = np.expand_dims(np.mean(y) * np.ones(y.shape[0]), axis=1)\n",
    "        total = y - y_mean\n",
    "        tss = np.squeeze(np.dot(total.transpose() , total))[()]\n",
    "        r_score = 1 - rss/tss\n",
    "        return r_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X_train, y_train, X_test, y_test, m):\n",
    "    gradient, cost, cost_t_values, cost_v_values = m.gradient, m.cost, m.cost_t_values, m.cost_v_values\n",
    "    alpha, beta = m.alpha, m.beta\n",
    "    params = np.ones((X_train.shape[1], 1))\n",
    "    cost_t_value = cost(y_train, np.dot(X_train, params))\n",
    "    step_numb = 0\n",
    "    max_step_inside = 10\n",
    "    while step_numb < m.steps:\n",
    "        step_size = m.step_size\n",
    "        y_pred = np.dot(X_train, params)\n",
    "        if m.benchmark_enabled:\n",
    "            prev_cost = cost_t_value\n",
    "            \n",
    "        grad = gradient(X_train, y_pred, y_train)\n",
    "        params -= step_size * grad\n",
    "        if m.benchmark_enabled or m.backtracking_enabled:\n",
    "            cost_t_value = cost(y_train, y_pred)\n",
    "            cost_t_values.append(cost_t_value)\n",
    "            cost_v_value = cost(y_test, np.dot(X_test, params))\n",
    "            cost_v_values.append(cost_v_value)\n",
    "    \n",
    "        # Backtracking to shrink step_size\n",
    "        if m.backtracking_enabled:\n",
    "            grad_norm = np.linalg.norm(grad)\n",
    "            margin = alpha * step_size * grad_norm**2\n",
    "            step_inside = 0\n",
    "            while cost_t_value > prev_cost - margin:\n",
    "                step_size *= beta\n",
    "                params -= step_size * grad\n",
    "                y_pred = np.dot(X_train, params)\n",
    "                cost_t_value = cost(y_train, y_pred)\n",
    "                margin = alpha * step_size * grad_norm**2\n",
    "                step_inside += 1\n",
    "                if step_inside > max_step_inside: break\n",
    "        if m.early_stopping_enabled and np.max(np.absolute(grad)) < m.tol: \n",
    "            break\n",
    "        step_numb += 1\n",
    "        \n",
    "    print(\"Total steps: \", step_numb)\n",
    "    print(\"Max abs component of grad: \", np.max(np.absolute(grad)))\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total steps:  19029\n",
      "Max abs component of grad:  0.000999899965561354\n"
     ]
    }
   ],
   "source": [
    "# best with step size: 11, steps: 150\n",
    "lreg_with_gd = LinearRegression(gradient_descent, 10e-2, 20000, benchmark_enabled=True, backtracking_enabled=True, tol=10e-4, early_stopping_enabled=True)\n",
    "lreg_with_gd.fit(X_train,y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "R^2 score on the training set:  0.7018678443949667\n",
      "R^2 score on the testing set:  0.6915378369160948\n",
      "Time for fitting:  15.69248342514038\n",
      "Cost on the training set:  4.05847445834084\n",
      "Cost on the testing set:  4.016679401258887\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAYbUlEQVR4nO3dfZBdd13H8ff33LsPye6mzcMSQtM2hWKHggplB6uUCpSnYqVaGacoUNAxI6KCjONUcYR/ZMSnUUcHJgpSpDwItENHpbRWBFEpbEKgKWmbtLQ2IWk2afOczd6Hr3+c3717H3fTe87Nvb/m85q5vef5fPfc9LO//Z1z7jF3R0RE4pMMugAREemNAlxEJFIKcBGRSCnARUQipQAXEYlU8WzubN26db5p06azuUsRkeht3br1oLtPt04/qwG+adMmZmdnz+YuRUSiZ2aPdZquLhQRkUgpwEVEIqUAFxGJlAJcRCRSCnARkUgpwEVEIqUAFxGJVBQBftu2Pdx6b8fLIEVEzllRBPgd3/0hn/v244MuQ0RkqEQR4AbouRMiIs3iCHCzQZcgIjJ0oghwAEdNcBGRRlEEuLpQRETaxRHgpgAXEWm1bICb2cfN7ICZ7WiYtsbM7jazXeF9dX/LNHWgiIi0OJMW+CeAN7RMuxm4x92fD9wTxvsmbYErwkVEGi0b4O7+deDJlsnXA7eE4VuAn8u3rGa6BkVEpF2vfeDr3X1fGN4PrO+2oJltNrNZM5udm5vrcXciItIq80lMT/s2uvZvuPsWd59x95np6bZHup0RncQUEWnXa4A/YWYbAML7gfxKameYrgMXEWnRa4DfAdwUhm8CvpRPOZ2pBS4i0u5MLiP8DPC/wGVmtsfMfhX4E+C1ZrYLeE0Y7xuzJfpoRETOUcXlFnD3t3SZdU3OtXRlmC4jFBFpEcWdmCIi0i6OAFcXiohImygC3EAJLiLSIo4AN30XiohIqzgCHH0XiohIqzgCXH3gIiJtogjwi049wAsrDwy6DBGRobLsdeDD4NqD/4gtHALeNehSRESGRhQt8Nq3oYiIyKIoAtzNMJ3EFBFpEkeA65EOIiJtoghwMBKqgy5CRGSoxBHgpha4iEirKALcdRJTRKRNFAEOKMBFRFpEEeBuCboXU0SkWRQBDkaiywhFRJrEEeBmqAUuItIsjgDXSUwRkTZRBLiDbuUREWkRR4BbgulGHhGRJlEEeNqFIiIijSIKcPWBi4g0yhTgZvYeM9thZveb2XtzqqnTjtBVKCIizXoOcDN7EfBrwMuAHweuM7NL8yqskasLRUSkTZYW+AuAe939pLuXga8BN+RTVjPXtxGKiLTJEuA7gFeY2VozWwm8EbiwdSEz22xms2Y2Ozc319ue1IUiItKm5wB3953Ah4G7gDuB7UClw3Jb3H3G3Wemp6d73Ju6UEREWmU6ienuH3P3l7r71cBTwEP5lNXCdBWKiEirTE+lN7NnufsBM7uItP/7ynzKapY+Uk0BLiLSKFOAA180s7VACXi3ux/OXlIHZiQKcBGRJpkC3N1fkVchS9NT6UVEWkVxJ6auAxcRaRdFgOsyQhGRdnEEuL4LRUSkTRwBbolOYoqItIgkwEFdKCIizSIJ8EQnMUVEWkQR4AbqAxcRaRFFgKctcAW4iEgjBbiISKTiCHDSLhTX3ZgiInVRBLiFk5hV5beISF0UAV67kaeqFriISF0cAZ4koQtl0IWIiAyPOAIc1AIXEWkRR4CbWuAiIq2iCHAzwklMJbiISE0kAZ6oC0VEpEUUAe71AB90JSIiwyOKADeMgulGHhGRRlEEePpEHt3IIyLSKIoAt1qAV6sDrkREZHhEEeCLLXAFuIhITaYAN7PfMbP7zWyHmX3GzMbzKqx5R2mZrj4UEZG6ngPczC4AfhuYcfcXAQXgxrwKa9kXAK4uFBGRuqxdKEVghZkVgZXAD7OX1Im6UEREWvUc4O6+F/hz4P+AfcARd7+rdTkz22xms2Y2Ozc319O+dBJTRKRdli6U1cD1wCXAc4AJM3tr63LuvsXdZ9x9Znp6usedJbVt9VquiMgzTpYulNcAP3D3OXcvAbcBP5VPWc3qfeDqQhERqcsS4P8HXGlmKy1N2GuAnfmU1SK0wKu6CkVEpC5LH/i9wBeAbcB9YVtbcqqrSWiAqw9cRKRBMcvK7v4B4AM51dKdFcL+Kn3flYhILKK4E9PqN/KoBS4iUhNFgC/2gasFLiJSE0eAJ6FMncQUEamLI8DrLfDygAsRERkecQR4Es616jpwEZG6KAJ88cus1AcuIlITRYCTpJcR6jpwEZFFUQT44mWEaoGLiNREEeC1FrgCXERkURQBbvU7MdWFIiJSE0mAh5OYFbXARURqogjw2mWEupVeRGRRJAFe+zpCtcBFRGqiCPD0kZvgqAUuIlITR4AntRt5dCu9iEhNFAG+2AeuL7MSEamJIsDrN/LogQ4iInVxBHj962QV4CIiNXEEeO1GHl1GKCJSF0WAoxa4iEibKALc6icxFeAiIjVRBHiS1E5iqgtFRKQmjgAv1L4PXC1wEZGangPczC4zs+0Nr6Nm9t4ca6tLal8nqy+zEhGpK/a6ors/CLwYwNLLRPYCt+dTVrNCQU/kERFplVcXyjXAw+7+WE7ba6IuFBGRdnkF+I3AZzrNMLPNZjZrZrNzc3M9bTzRE3lERNpkDnAzGwXeBHy+03x33+LuM+4+Mz093dM+ai1wBbiIyKI8WuDXAtvc/YkcttVRQV0oIiJt8gjwt9Cl+yQvhYKeyCMi0ipTgJvZBPBa4LZ8yumsfiOPWuAiInU9X0YI4O4ngLU51dJVoahb6UVEWkVxJ2ZBDzUWEWkTRYAn4ZFqOokpIrIoigCvfxuhnsgjIlIXRYBjOokpItIqjgAPd2KiPnARkbo4AlwtcBGRNpEEeLiVXg90EBGpiyTA06tQ9ExMEZFFcQR4uArFFOAiInVxBHhhJH2vlgZbh4jIEIkjwJM0wE3XgYuI1EUS4OlJTKuWB1yIiMjwiCPAzShTIFEXiohIXRwBDpQpgKsFLiJSE02AVymQqAtFRKQumgAvWxFTC1xEpC6aAK9YUdeBi4g0iCbAqyQkaoGLiNRFE+AVK6oPXESkQVQBrj5wEZFF0QR4VS1wEZEm0QS4WwF0K72ISF2mADez883sC2b2gJntNLOfzKuwVtWkSEFdKCIidcWM6/81cKe7v9nMRoGVOdTUUdVG1IUiItKg5wA3s/OAq4F3ALj7ArCQT1kdJAUMdaGIiNRk6UK5BJgD/tHMvmNm/2BmE60LmdlmM5s1s9m5ubmed+amLhQRkUZZArwIXAF8xN1fApwAbm5dyN23uPuMu89MT0/3vDNPRhTgIiINsgT4HmCPu98bxr9AGuj9kRRJdBWKiEhdzwHu7vuBx83ssjDpGuD7uVTVaX9JkSIVKlXv1y5ERKKS9SqU3wJuDVegPAK8M3tJXRRGKFKmVKlSCE/oERE5l2UKcHffDszkU8oykgIFqixUqoyPKMBFRKK5E5PCCCOUWShXB12JiMhQiCfAk1FGLe1CERGRmAK8OMYoJbXARUSCaALciysYo6QWuIhIEE2A28gYY5Q4rRa4iAgQVYCPU7QqpVJp0KWIiAyFaAI8KY4DsDB/YsCViIgMh2gCvDC6AoCF+ZMDrkREZDhEE+AjY7UAPzXgSkREhkM0AV5UgIuINIkmwEfG04f9lE6rC0VEBCIK8LGxNMDLC2qBi4hARAE+Op52oZROK8BFRCCiAE9G0gCvqgUuIgJEFOAUxwCoKMBFRICoAjy9kadamh9wISIiwyGiAE9b4F5SC1xEBGIK8NFJAKykW+lFRCCmAB+bAqCgABcRAWIK8OIYZQokpeODrkREZCjEE+BmLCQrKaoFLiICxBTgwEJxgmJZAS4iApEFeLk4wXj1JGU9Vk1EhGKWlc3sUeAYUAHK7j6TR1HdVEYmmeAUR+fLrJkY7eeuRESGXqYAD17l7gdz2M7yxiaZsv0cOVVSgIvIOS+qLhQbm2KCeY6c0nMxRUSyBrgDd5nZVjPb3GkBM9tsZrNmNjs3N5dpZ8n4KibtlAJcRITsAX6Vu18BXAu828yubl3A3be4+4y7z0xPT2faWXFiDas5xqFj+j4UEZFMAe7ue8P7AeB24GV5FNXNitXrGbcSTx4+3M/diIhEoecAN7MJM5uqDQOvA3bkVVgnY6vWA3DiqX393I2ISBSyXIWyHrjdzGrb+bS735lLVd1MrANg4ciBvu5GRCQGPQe4uz8C/HiOtSxvZRrg5WPZToaKiDwTRHUZIRNr0/eTZ+eycxGRYRZZgKdXsRRPHqSk2+lF5BwXV4CPTrAwsor1HGLPU3oyj4ic2+IKcKA8dSEX2gEemdP3govIuS26AC+u28RGO8gjc/paWRE5t0UX4KNrL+HCZI6H9h8ddCkiIgMVXYCzehPjLLDn8UcGXYmIyEDFF+DrXwjA2JM7OTqvL7USkXNXtAF+OY+y7bGnBlyMiMjgxBfg4+dRPX8TP1Z8jHt26pZ6ETl3xRfgQLJxhp8sPsTd9++nWvVBlyMiMhBRBjjPezXnVZ7i/OO7+NoufS+KiJybIg3wVwFw/Yrv8rH/+sGAixERGYw4A3zVc+Diq/ilsW/wjd1z3LPziUFXJCJy1sUZ4ABXvJ3zTj3O21bv5Obb7mP/ET1mTUTOLfEG+ItugDXP5Q/HP8/C6Xne9rF72XdEX3AlIueOeAO8MAKv/xBjTz3InS+4i31HTnHd33yDO3fsw11XpojIM1+8AQ5w2bXwE+9iw4O38PUrvs6GVaP8+qe2ccNH/od/u28fp8uVQVcoItI3WZ6JORxe/yEoz7Nm699xx6bv8eUXvo8PzZ7mN27dxnkrRnjlZdNcdek6rnzuWjauXkF4hqeISPTsbHY3zMzM+OzsbP4bdofvfAq+8gewcJzqC65nx7Nv4JYfbuRruw9x8PgCAOetGOHyDau47NlTXLx2JReuXsmFa1ZyweoVTIwWFO4iMpTMbKu7z7RNf0YEeM3JJ+G//wq23gLzh2FyPX7pa9m77uV8q/Q8vv3kCr6/7ygPPXGcU6Xm7pXxkYR1k2OsnRxj3cQoaydHWTU+wtT4CJPjRabGikyNF5kcLzIZhsdHCouvYkKxEHePlIgMp3MjwGtK8/DAv8AD/wq774HTR9Lpk8+G57wYX3spx6cuYV9xIz+oTPPo/CQHT1Y4dHyBgycWOHT8NAePn+bYfJmTC2fej15MjBUjBcZGCoyPJCHcE8aLaciPFIyRQsJIMWG0kCyOtwyPFhfHi4WE0abl0nmFxCgmCUkCxSShkNSmLb4nDeOL89uXLVi6rIgMp24BnrkP3MwKwCyw192vy7q9XIyMw4++OX1VSrD/e7BnK+ydhf33YQ9/lanKaaaAHwHAYPJZMLUhfa1dC+Pnw4rzqYydx+nCKk4UJjmRTHG8OsqxyihHKyOc8DGOV0eYLzvzpSrzpQrzpSqnShVOlyrMlyv16ScXypQqTqlSDa/F4YXy4nh5QN/tYkZToBcSo1hISMwoJJCYkZhhRpi2OJyEd2sYToz6eLps47zu6xaSpecnZiRJum0LdRsW3ql3g7VPX1yHTvMaxum6bZq62TrOD+PQWmMYP5N9UFsobKfts7KO8xp7AFt7Aw3rOu/pbnup7S21DmewztOqoWkbnTfY87bb1uy+bPN63b34ovNZOZrvacc8tvYeYCewKodt5a8wAhe8NH2xOZ1WrcCRPXBoFxx+HI7tg6M/hGP74cjjsG87nDoM5VMUgJXhNd1tH8VxGFkJoxPpe3EMCqPpqzgKK0cbxsfSmgqjUAjDDdPcEioUqWBUKFAmoeJJfbjkCWVPqFqBCgllL1C1dFptvIJRpkCZAhU3Sp5QcaPsRrXqlN0oO+l2q07JoVw1Km5UqqTjbpSrUK5CFaNSWx6oVNPhKunph6p7eIGH96o7laq3za9Uq/X5i8s7lericG1+07rVxW1X3PEw30mne6ilNkyneSyuQ8t443Ii/fDv7/tpLn3WZK7bzBTgZrYR+Bngj4H35VLR2ZAUYPXF6Wsp5dNpkM8fhlNPwfwRKJ2EhZPpe334RHg/lQ6XF6ByOm39L5yEyuF0uHIaKgth/kLztMBIP5R4Lg8ysKTh1TJeb03Wm6WL653puAFJr+vTZf7S2/CmlZvbVeFXRG2k9h/ahjr+MmhY25un14e6TG9k7l3mdF+n+2+nSJZfcp0u67Yt3jq/0/aap1mnetrWW3675co9wGUd9te7rDnxV8DvAVPdFjCzzYSm70UXXZRxd2dZcQym1qevfnIPYb4AXkn/QqhWoFoO4+Xlp9WnN86rTa+GJmg1fdEw3DTPl5kfhpecX1u/dR2o/yPPfZynuXzj/1ydl7FO69S6Z2jV5Q/nrn9r93v5QdbU53p6qqmtI+UMtpfHMs3jxanVnevLoOcAN7PrgAPuvtXMXtltOXffAmyB9CRmr/t7RjNLu1qKo4OuREQikuW6t5cDbzKzR4HPAq82s0/lUpWIiCyr5wB39993943uvgm4EfgPd39rbpWJiMiSdOeJiEikcrnYwd3/E/jPPLYlIiJnRi1wEZFIKcBFRCKlABcRiZQCXEQkUmf12wjNbA54rMfV1wEHcywnb6ovG9WXjerLZtjru9jd276O6awGeBZmNtvp6xSHherLRvVlo/qyGfb6ulEXiohIpBTgIiKRiinAtwy6gGWovmxUXzaqL5thr6+jaPrARUSkWUwtcBERaaAAFxGJVBQBbmZvMLMHzWy3md18lvZ5oZl91cy+b2b3m9l7wvQPmtleM9seXm9sWOf3Q40Pmtnr+12/mT1qZveFOmbDtDVmdreZ7Qrvq8N0M7O/CTV8z8yuaNjOTWH5XWZ2U061XdZwjLab2VEze+8gj5+ZfdzMDpjZjoZpuR0vM3tp+Dx2h3WXemTOmdb3Z2b2QKjhdjM7P0zfZGanGo7jR5ero9vPmrG+3D5PM7vEzO4N0z9nZk/rCSdd6vtcQ22Pmtn2QR2/vvD6A2KH8wUUgIeB5wKjwHeBy8/CfjcAV4ThKeAh4HLgg8Dvdlj+8lDbGHBJqLnQz/qBR4F1LdP+FLg5DN8MfDgMvxH4Mulznq4E7g3T1wCPhPfVYXh1Hz7D/cDFgzx+wNXAFcCOfhwv4FthWQvrXptDfa8DimH4ww31bWpcrmU7Hevo9rNmrC+3zxP4Z+DGMPxR4F1Z62uZ/xfAHw3q+PXjFUML/GXAbnd/xN0XSJ/+c32/d+ru+9x9Wxg+BuwELlhileuBz7r7aXf/AbCbtPazXf/1wC1h+Bbg5xqmf9JT3wTON7MNwOuBu939SXd/CrgbeEPONV0DPOzuS92F2/fj5+5fB57ssN/MxyvMW+Xu3/T0//BPNmyr5/rc/S53L4fRbwIbl9rGMnV0+1l7rm8JT+vzDK3cVwNf6Ed9Yfu/CHxmqW308/j1QwwBfgHweMP4HpYO0tyZ2SbgJcC9YdJvhj9pP97wZ1S3OvtZvwN3mdlWSx8eDbDe3feF4f1A7YnMg6iv5kaa/8cZluMH+R2vC8Jwv+oE+BXSFmHNJWb2HTP7mpm9oqHubnV0+1mzyuPzXAscbvhllffxewXwhLvvapg2LMevZzEE+ECZ2STwReC97n4U+AjwPODFwD7SP8sG5Sp3vwK4Fni3mV3dODO0IAZ6nWjox3wT8PkwaZiOX5NhOF7dmNn7gTJwa5i0D7jI3V8CvA/4tJmtOtPt5fizDu3n2eItNDcihuX4ZRJDgO8FLmwY3xim9Z2ZjZCG963ufhuAuz/h7hV3rwJ/T/on4VJ19q1+d98b3g8At4dangh/Btb+HDwwqPqCa4Ft7v5EqHVojl+Q1/HaS3P3Rm51mtk7gOuAXw7BQeiaOBSGt5L2K//IMnV0+1l7luPneYi0m6rYMj2zsM0bgM811D0Uxy+rGAL828DzwxnqUdI/x+/o905Dn9nHgJ3u/pcN0zc0LPbzQO2M9x3AjWY2ZmaXAM8nPRnSl/rNbMLMpmrDpCe7doRt166MuAn4UkN9b7fUlcCR8OfgV4DXmdnq8Ofv68K0vDS1fIbl+DXI5XiFeUfN7Mrwb+ftDdvqmZm9Afg94E3ufrJh+rSZFcLwc0mP1yPL1NHtZ81SXy6fZ/jF9FXgzXnWF7wGeMDd610jw3L8Mhv0WdQzeZFeEfAQ6W/J95+lfV5F+ifS94Dt4fVG4J+A+8L0O4ANDeu8P9T4IA1XIPSjftKz+N8Nr/tr2yXtS7wH2AX8O7AmTDfg70IN9wEzDdv6FdKTTLuBd+Z4DCdIW1bnNUwb2PEj/UWyDyiR9m3+ap7HC5ghDbCHgb8l3Omcsb7dpH3GtX+DHw3L/kL43LcD24CfXa6Obj9rxvpy+zzDv+lvhZ/588BY1vrC9E8Av96y7Fk/fv146VZ6EZFIxdCFIiIiHSjARUQipQAXEYmUAlxEJFIKcBGRSCnARUQipQAXEYnU/wMWEcWGL5cJCgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "conclusion(lreg_with_gd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accelerated gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LogisticRegression' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# best with step size 0.5, steps: 5\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m logreg_with_sgd \u001b[38;5;241m=\u001b[39m \u001b[43mLogisticRegression\u001b[49m(stochastic_gradient_descent, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m      3\u001b[0m conclusion(logreg_with_sgd)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LogisticRegression' is not defined"
     ]
    }
   ],
   "source": [
    "# best with step size 0.5, steps: 5\n",
    "logreg_with_sgd = LogisticRegression(stochastic_gradient_descent, 0.5, 5)\n",
    "conclusion(logreg_with_sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton(X_train, y_train, X_test, y_test, m):\n",
    "    gradient, cost, cost_t_values, cost_v_values = m.gradient, m.cost, m.cost_t_values, m.cost_v_values\n",
    "    alpha, beta = m.alpha, m.beta\n",
    "    params = np.ones((X_train.shape[1], 1))\n",
    "    cost_t_value = cost(y_train, np.dot(X_train, params))\n",
    "    step_numb = 0\n",
    "    max_step_inside = 10\n",
    "    while step_numb < m.steps:\n",
    "        hess = m.hessian(X_train)\n",
    "        inv_hess = np.linalg.inv(hess)\n",
    "        y_pred = np.dot(X_train, params)\n",
    "        if m.benchmark_enabled:\n",
    "            prev_cost = cost_t_value\n",
    "            \n",
    "        grad = gradient(X_train, y_pred, y_train)\n",
    "        params -= np.dot(inv_hess , grad)\n",
    "        print(step_size.shape, grad.shape, params.shape)\n",
    "        if m.benchmark_enabled or m.backtracking_enabled:\n",
    "            cost_t_value = cost(y_train, y_pred)\n",
    "            cost_t_values.append(cost_t_value)\n",
    "            cost_v_value = cost(y_test, np.dot(X_test, params))\n",
    "            cost_v_values.append(cost_v_value)\n",
    "    \n",
    "        # Backtracking to shrink step_size\n",
    "        if m.backtracking_enabled:\n",
    "            grad_norm = np.linalg.norm(grad)\n",
    "            margin = alpha * step_size * grad_norm**2\n",
    "            step_inside = 0\n",
    "            step_size = 1\n",
    "            while cost_t_value > prev_cost - margin:\n",
    "                step_size *= beta\n",
    "                params -= step_size * np.dot(inv_hess , grad)\n",
    "                y_pred = np.dot(X_train, params)\n",
    "                cost_t_value = cost(y_train, y_pred)\n",
    "                margin = alpha * step_size * grad_norm**2\n",
    "                step_inside += 1\n",
    "                if step_inside > max_step_inside: break\n",
    "        if m.early_stopping_enabled and np.max(np.absolute(grad)) < m.tol: \n",
    "            break\n",
    "        step_numb += 1\n",
    "        \n",
    "    print(\"Total steps: \", step_numb)\n",
    "    print(\"Max abs component of grad: \", np.max(np.absolute(grad)))\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19, 19) (19, 1) (19, 1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [29]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# best with step size: 5\u001b[39;00m\n\u001b[1;32m      2\u001b[0m lreg_with_newton \u001b[38;5;241m=\u001b[39m LinearRegression(newton, \u001b[38;5;241m10e-2\u001b[39m, \u001b[38;5;241m20000\u001b[39m, benchmark_enabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, backtracking_enabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, tol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10e-4\u001b[39m, early_stopping_enabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mlreg_with_newton\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36mLinearRegression.fit\u001b[0;34m(self, X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[1;32m     20\u001b[0m X_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((X_train, np\u001b[38;5;241m.\u001b[39mones((X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m))), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     21\u001b[0m X_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((X_test, np\u001b[38;5;241m.\u001b[39mones((X_test\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m))), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfitting_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36mnewton\u001b[0;34m(X_train, y_train, X_test, y_test, m)\u001b[0m\n\u001b[1;32m     27\u001b[0m margin \u001b[38;5;241m=\u001b[39m alpha \u001b[38;5;241m*\u001b[39m step_size \u001b[38;5;241m*\u001b[39m grad_norm\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     28\u001b[0m step_inside \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m cost_t_value \u001b[38;5;241m>\u001b[39m prev_cost \u001b[38;5;241m-\u001b[39m margin:\n\u001b[1;32m     30\u001b[0m     step_size \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m beta\n\u001b[1;32m     31\u001b[0m     params \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(step_size , grad)\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "# best with step size: 5\n",
    "lreg_with_newton = LinearRegression(newton, 10e-2, 20000, benchmark_enabled=True, backtracking_enabled=True, tol=10e-4, early_stopping_enabled=True)\n",
    "lreg_with_newton.fit(X_train,y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:common]",
   "language": "python",
   "name": "conda-env-common-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
